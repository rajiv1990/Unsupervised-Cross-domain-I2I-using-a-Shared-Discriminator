{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy.random as random\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import init\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from cv2 import imread\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "from math import exp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train' #help=train or test\n",
    "exp_name = 'horses2zebras' # name of the experiment\n",
    "#exp_name = 'apples2oranges'\n",
    "#exp_name = 'catsvsdogs'\n",
    "#exp_name = 'cityscapes'\n",
    "n_epochs = 200 #help=number of epochs of training\n",
    "batchsize = 4 #help=size of the batches\n",
    "lr = 0.0001 #help=adam: learning rate\n",
    "b1 = 0.5 #help=adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 #help=adam: decay of first order momentum of gradient\n",
    "n_cpu = 4 #help=number of cpu threads to use during batch generation\n",
    "img_size = 256 #help=size of each image dimension\n",
    "channels = 3 #help=number of image channels\n",
    "img_shape = channels, img_size, img_size # can be manually set if needed\n",
    "print(np.prod(img_shape),img_shape)\n",
    "load = False # help='load file model' Useful for resuming training from a checkpoint\n",
    "scale = 0.5 # help='downscaling factor of the images')\n",
    "save_epoch_freq = 10 # help='frequency of saving checkpoints at the end of epochs'\n",
    "dir_checkpoint = './checkpoints' # directory to save the checkpoints\n",
    "pool_size = 32 * batchsize # help = set the pool size. Can be set to different sizes for different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_seed = 0 \n",
    "torch.manual_seed(init_seed)\n",
    "np.random.seed(init_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "if(cuda):\n",
    "    torch.cuda.manual_seed(init_seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 'batch' # the name of the normalization layer: batch | instance | none\n",
    "init_type = 'normal' # the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "init_gain = 0.02 # scaling factor for normal, xavier and orthogonal.\n",
    "lambda_criterionGAN = 10.0 # weight \n",
    "lambda_L1_real = 200.0 #\n",
    "lambda_L1_fake = 10.0 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help = assumes that the dataset is available in the dir: ./data/'task'/ [train,test].pt\n",
    "# use the helper function at the end of the notebook to generate dataset in the required format\n",
    "def dataset_memory_loader(task, ops, dir_data  = './data/'):\n",
    "    for op in ops:\n",
    "        images = []\n",
    "        file_names = []\n",
    "        path = dir_data+task+'/'+op\n",
    "        print(path)\n",
    "        count = 0\n",
    "        for file in os.listdir(path):\n",
    "            img_path = path+'/'+file\n",
    "            file_names.append(img_path)\n",
    "            img = pil_loader(img_path)\n",
    "            img_int = np.array(img,dtype = 'uint8')\n",
    "            images.append(img_int)\n",
    "        count = count + 1\n",
    "        return file_names, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_dataset(torch.utils.data.Dataset):\n",
    "    #The dataset is saved as dictionary files on disks\n",
    "    #Merge the dictionary files depending upon the size of RAM before passing in\n",
    "    #Dictionary key: path of the file; item: Image file loaded as uint8 bytes using PIL Image\n",
    "    \n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    def __init__(self, data_dict, transform=True):\n",
    "        if type(data_dict) is dict:\n",
    "            self.data = data_dict\n",
    "        self.keys = []\n",
    "        for i in self.data.keys():\n",
    "            self.keys.append(i)\n",
    "        if(transform is True):\n",
    "            self.transform = True\n",
    "        else: \n",
    "            self.transform = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return np.size(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        label = re.split('/',key)[3].split('.')[0]\n",
    "        image = self.data.get(key)\n",
    "        if (self.transform is True):\n",
    "            image = generic_transforms(image,resize=int(img_size*1.125), randomcrop=img_size, toTensor=True)   \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_dataset_test(torch.utils.data.Dataset):\n",
    "    #The dataset is saved as dictionary files on disks\n",
    "    #Merge the dictionary files depending upon the size of RAM before passing in\n",
    "    #Dictionary key: path of the file; item: Image file loaded as uint8 bytes using PIL Image\n",
    "    \n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    def __init__(self, data_dict, transform=True):\n",
    "        if type(data_dict) is dict:\n",
    "            self.data = data_dict\n",
    "        self.keys = []\n",
    "        for i in self.data.keys():\n",
    "            self.keys.append(i)\n",
    "        if(transform is True):\n",
    "            self.transform = True\n",
    "        else: \n",
    "            self.transform = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return np.size(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        label = re.split('/',key)[3].split('.')[0]\n",
    "        image = self.data.get(key)\n",
    "        if (self.transform is True):\n",
    "            image = generic_transforms(image, toTensor=True)   \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_paired_dataset(torch.utils.data.Dataset):\n",
    "    #The dataset is saved as dictionary files on disks\n",
    "    #Merge the dictionary files depending upon the size of RAM before passing in\n",
    "    #Dictionary key: path of the file; item: Image file loaded as uint8 bytes using PIL Image\n",
    "    \n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    def __init__(self, data_dict, transform=True):\n",
    "        if type(data_dict) is dict:\n",
    "            self.data = data_dict\n",
    "        self.keys = []\n",
    "        for i in self.data.keys():\n",
    "            self.keys.append(i)\n",
    "        if(transform is True):\n",
    "            self.transform = True\n",
    "        else: \n",
    "            self.transform = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return np.size(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        label = re.split('/',key)[3].split('.')[0]\n",
    "        image = self.data.get(key)\n",
    "        if (self.transform is True):\n",
    "            # split AB image into A and B\n",
    "            height, width = image.shape[0], image.shape[1]\n",
    "            w2 = int(width / 2)\n",
    "            image_A = image[:,:w2,:]\n",
    "            image_B = image[:,w2:,:]\n",
    "            image_A = generic_transforms(image_A,resize=int(img_size*1.125), randomcrop=img_size, toTensor=False)   \n",
    "            image_B = generic_transforms(image_B,resize=int(img_size*1.125), randomcrop=img_size, toTensor=False)   \n",
    "        return (image_A,image_B), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_transforms(image, resize = None, randomcrop = None, means = None, stds = None, hflip = False, toTensor = True):\n",
    "    import torchvision.transforms.functional as TF\n",
    "    import random\n",
    "    from PIL import Image\n",
    "    \n",
    "    image = Image.fromarray(image)\n",
    "    \n",
    "    if(resize is not None):\n",
    "        image = TF.resize(image, resize)\n",
    "\n",
    "    if(randomcrop is not None):\n",
    "        T = torchvision.transforms.transforms.RandomCrop(size=randomcrop)\n",
    "        image = T.__call__(img=image)\n",
    "\n",
    "    if(hflip is True):\n",
    "        if(random.random() > 0.5):\n",
    "            image = TF.hflip(image)\n",
    "            \n",
    "    if(toTensor is True):\n",
    "        image = TF.to_tensor(image)\n",
    "    else:\n",
    "        image = np.array(image)\n",
    "        \n",
    "    if(means is not None and stds is not None):\n",
    "        image = TF.normalize(image,means,stds)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_paired_dataset(torch.utils.data.Dataset):\n",
    "    #The dataset is saved as dictionary files on disks\n",
    "    #Merge the dictionary files depending upon the size of RAM before passing in\n",
    "    #Dictionary key: path of the file; item: Image file loaded as uint8 bytes using PIL Image\n",
    "    \n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    def __init__(self, data_dict, transform=True):\n",
    "        if type(data_dict) is dict:\n",
    "            self.data = data_dict\n",
    "        self.keys = []\n",
    "        for i in self.data.keys():\n",
    "            self.keys.append(i)\n",
    "        if(transform is True):\n",
    "            self.transform = True\n",
    "        else: \n",
    "            self.transform = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return np.size(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        label = re.split('/',key)[3].split('.')[0]\n",
    "        image = self.data.get(key)\n",
    "        if (self.transform is True):\n",
    "            # split AB image into A and B\n",
    "            height, width = image.shape[0], image.shape[1]\n",
    "            w2 = int(width / 2)\n",
    "            image_A = image[:,:w2,:]\n",
    "            image_B = image[:,w2:,:]\n",
    "            image_A = generic_transforms(image_A,resize=int(img_size*1.125), randomcrop=img_size, toTensor=False)   \n",
    "            image_B = generic_transforms(image_B,resize=int(img_size*1.125), randomcrop=img_size, toTensor=False)   \n",
    "        return (image_A,image_B), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to be applied if using custom image folder, when dataset is large enough to not fit in memory\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize(int(img_size*1.125)),\n",
    "    transforms.RandomCrop(img_size), \n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imagefolder_(torchvision.datasets.ImageFolder):\n",
    "    def __init__(self, root='./',transform= transforms_):\n",
    "        super(Imagefolder_, self).__init__(root, transform = transforms_)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dataset_train_A = Imagefolder_('./datasets/catsvsdogs/dogs/')\n",
    "dataset_train_B = Imagefolder_('./datasets/catsvsdogs/cats/')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dataloader_train_A = torch.utils.data.DataLoader(\n",
    "    dataset_train_A,\n",
    "    batch_size=batchsize,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "dataloader_train_B = torch.utils.data.DataLoader(\n",
    "    dataset_train_B,\n",
    "    batch_size= batchsize,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset apples2oranges\n",
    "if(mode=='train'):\n",
    "    data_dict_train_A = torch.load('./apples2oranges/trainA.pt')\n",
    "    data_dict_train_B = torch.load('./apples2oranges/trainB.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset horse2zebra\n",
    "if(mode=='train'):\n",
    "    data_dict_train_A = torch.load('./horse2zebra/trainA.pt')\n",
    "    data_dict_train_B = torch.load('./horse2zebra/trainB.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if(mode=='train'):\n",
    "    data_dict_train_A = torch.load('./datasets/Imagenet/Black_bear/train.pt')\n",
    "    data_dict_train_B = torch.load('./datasets/Imagenet/Polar_bear/train.pt')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if(mode=='train'):\n",
    "    data_dict_train_A = torch.load('./cityscapes/trainA.pt')\n",
    "    data_dict_train_B = torch.load('./cityscapes/trainB.pt')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if(mode=='test'):\n",
    "    data_dict_test_A = torch.load('./cityscapes/testA.pt')\n",
    "    data_dict_test_B = torch.load('./cityscapes/testB.pt')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute in the training mode\n",
    "if(mode=='train'):\n",
    "    dataset_train_A = generic_dataset(\n",
    "        data_dict=data_dict_train_A,\n",
    "        transform=True\n",
    "    )\n",
    "\n",
    "    dataset_train_B = generic_dataset(\n",
    "        data_dict=data_dict_train_B,\n",
    "        transform=True\n",
    "    )\n",
    "\n",
    "    dataloader_train_A = torch.utils.data.DataLoader(\n",
    "        dataset_train_A,\n",
    "        batch_size=batchsize,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    dataloader_train_B = torch.utils.data.DataLoader(\n",
    "        dataset_train_B,\n",
    "        batch_size= batchsize,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train_A.__len__(),dataset_train_B.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute in the testing mode\n",
    "assert(mode == 'test')\n",
    "\n",
    "# Use one of the datasets for testing. Assumes the dataset is available in the .pt format\n",
    "if(mode=='test'):\n",
    "    #data_dict_test_A = torch.load('./apples2oranges/testA.pt')\n",
    "    #data_dict_test_B = torch.load('./apples2oranges/testB.pt')\n",
    "    data_dict_test_A = torch.load('./horse2zebra/testA.pt')\n",
    "    data_dict_test_B = torch.load('./horse2zebra/testB.pt')\n",
    "\n",
    "if(mode=='test'):\n",
    "    dataset_test_A = generic_dataset_test(\n",
    "        data_dict=data_dict_test_A,\n",
    "        transform=True\n",
    "    )\n",
    "\n",
    "    dataset_test_B = generic_dataset_test(\n",
    "        data_dict=data_dict_test_B,\n",
    "        transform=True\n",
    "    )\n",
    "\n",
    "    dataloader_test_A = torch.utils.data.DataLoader(\n",
    "        dataset_test_A,\n",
    "        batch_size=batchsize,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    dataloader_test_B = torch.utils.data.DataLoader(\n",
    "        dataset_test_B,\n",
    "        batch_size= batchsize,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            user_dropout (bool) -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet generator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "            ngf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "\n",
    "        We construct the U-Net from the innermost layer to the outermost layer.\n",
    "        It is a recursive process.\n",
    "        \"\"\"\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_layer(norm_type='instance'):\n",
    "    \"\"\"Return a normalization layer\n",
    "    Parameters:\n",
    "        norm_type (str) -- the name of the normalization layer: batch | instance | none\n",
    "    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n",
    "    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n",
    "    \"\"\"\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = lambda x: Identity()\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=0):\n",
    "    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n",
    "    Parameters:\n",
    "        net (network)      -- the network to be initialized\n",
    "        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Return an initialized network.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    assert(torch.cuda.is_available())\n",
    "    net.to(device)\n",
    "    #net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(netG, ngf=64, input_nc=3, output_nc=3, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[0]):\n",
    "    \"\"\"Create a generator\n",
    "\n",
    "    Parameters:\n",
    "        input_nc (int) -- the number of channels in input images\n",
    "        output_nc (int) -- the number of channels in output images\n",
    "        ngf (int) -- the number of filters in the last conv layer\n",
    "        netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128\n",
    "        norm (str) -- the name of normalization layers used in the network: batch | instance | none\n",
    "        use_dropout (bool) -- if use dropout layers.\n",
    "        init_type (str)    -- the name of our initialization method.\n",
    "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Returns a generator\n",
    "\n",
    "    Our current implementation provides two types of generators:\n",
    "        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)\n",
    "        The original U-Net paper: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\n",
    "        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\n",
    "        We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\n",
    "\n",
    "    The generator has been initialized by <init_net>. It uses RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netG == 'resnet_9blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n",
    "    elif netG == 'resnet_6blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n",
    "    elif netG == 'unet_128':\n",
    "        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    elif netG == 'unet_256':\n",
    "        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    else:\n",
    "        raise NotImplementedError('Generator model name [%s] is not recognized' % netG)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(PixelDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        self.net = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
    "            norm_layer(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    \"\"\"This class implements an image buffer that stores previously generated images.\n",
    "\n",
    "    This buffer enables us to update discriminators using a history of generated images\n",
    "    rather than the ones produced by the latest generators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        \"\"\"Initialize the ImagePool class\n",
    "\n",
    "        Parameters:\n",
    "            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n",
    "        \"\"\"\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:  # create an empty pool\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        \"\"\"Return an image from the pool.\n",
    "\n",
    "        Parameters:\n",
    "            images: the latest generated images from the generator\n",
    "\n",
    "        Returns images from the buffer.\n",
    "\n",
    "        By 50/100, the buffer will return input images.\n",
    "        By 50/100, the buffer will return images previously stored in the buffer,\n",
    "        and insert the current images to the buffer.\n",
    "        \"\"\"\n",
    "        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n",
    "            return images\n",
    "        return_images = []\n",
    "        \n",
    "        for image in images:\n",
    "            image = torch.unsqueeze(image.data, 0)\n",
    "            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n",
    "                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:       # by another 50% chance, the buffer will return the current image\n",
    "                    return_images.append(image)\n",
    "        return_images = torch.cat(return_images, 0)   # collect all the images and return\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"Set requies_grad=False for all the networks to avoid unnecessary computations\n",
    "    Parameters:\n",
    "        nets (network list)   -- a list of networks\n",
    "        requires_grad (bool)  -- whether the networks require gradients or not\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Define different GAN objectives.\n",
    "    The GANLoss class abstracts away the need to create the target label tensor\n",
    "    that has the same size as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "        \"\"\" Initialize the GANLoss class.\n",
    "\n",
    "        Parameters:\n",
    "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
    "            target_real_label (bool) - - label for a real image\n",
    "            target_fake_label (bool) - - label of a fake image\n",
    "\n",
    "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
    "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode in ['wgangp']:\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        \"\"\"Create label tensors with the same size as the input.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            A label tensor filled with ground truth label, and with the size of the input\n",
    "        \"\"\"\n",
    "\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(mode == 'train'):\n",
    "    if(img_size == 128):\n",
    "        arch = 'unet_128'\n",
    "    elif(img_size == 256):\n",
    "        arch = 'unet_256'\n",
    "    \n",
    "    net_adversarial_1 = NLayerDiscriminator()\n",
    "    net_GA = define_G(netG = arch, norm='batch')\n",
    "    net_GB = define_G(netG = arch, norm='batch')\n",
    "\n",
    "    if cuda:\n",
    "        net_adversarial_1.cuda()\n",
    "        net_GA.cuda()\n",
    "        net_GB.cuda()\n",
    "else:\n",
    "    if(mode == 'test'):\n",
    "        arch = 'unet_128'\n",
    "        net_GA = define_G(netG = arch, norm='batch')\n",
    "        net_GB = define_G(netG = arch, norm='batch')\n",
    "        \n",
    "        if cuda:\n",
    "            net_GA.cuda()\n",
    "            net_GB.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if((img_size == 256) and (mode == 'test')):\n",
    "    arch = 'unet_256'\n",
    "    \n",
    "    net_GA = define_G(netG = arch, norm='batch')\n",
    "    net_GB = define_G(netG = arch, norm='batch')\n",
    "\n",
    "    if cuda:\n",
    "        net_GA.cuda()\n",
    "        net_GB.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_GA = torch.optim.Adam(net_GA.parameters(), lr = lr, betas=(b1, b2))\n",
    "optimizer_GB = torch.optim.Adam(net_GB.parameters(), lr = lr, betas=(b1, b2))\n",
    "optimizer_adversarial_1 = torch.optim.Adam(net_adversarial_1.parameters(), lr = lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate optimizer\n",
    "'''lr = 0.001\n",
    "optimizer_GA = torch.optim.RMSprop(net_GA.parameters(), lr = lr)\n",
    "optimizer_GB = torch.optim.RMSprop(net_GB.parameters(), lr = lr)\n",
    "optimizer_adversarial_1 = torch.optim.RMSprop(net_adversarial_1.parameters(), lr = lr)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the date_time of the experiment to load the checkpoints\n",
    "date_time = 'Tue Sep  8 13:13:39 2020' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved checkpoints\n",
    "model_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/net_GA_500.pth')\n",
    "net_GA.load_state_dict(model_saved)\n",
    "model_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/net_GB_500.pth')\n",
    "net_GB.load_state_dict(model_saved)\n",
    "model_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/net_adversarial_1_500.pth')\n",
    "net_adversarial_1.load_state_dict(model_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved optimizers\n",
    "optimizer_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/optimizer_GA_500.pth')\n",
    "optimizer_GA.load_state_dict(optimizer_saved)\n",
    "optimizer_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/optimizer_GB_500.pth')\n",
    "optimizer_GB.load_state_dict(optimizer_saved)\n",
    "optimizer_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/optimizer_adversarial_1_500.pth')\n",
    "optimizer_adversarial_1.load_state_dict(optimizer_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan_mode= 'wgangp'\n",
    "gan_mode= 'vanilla'\n",
    "criterionGAN = GANLoss(gan_mode).cuda()\n",
    "criterionL1 = torch.nn.L1Loss().cuda()\n",
    "criterionL2 = torch.nn.MSELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(imgs, save = False):\n",
    "    images = imgs.detach().cpu().numpy()\n",
    "    images = np.transpose(images, axes=(0,2,3,1))\n",
    "    if(save):\n",
    "        try:\n",
    "            os.mkdir('Imgs')\n",
    "        except:\n",
    "            print('Folder already exists')\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.imshow(images[i])\n",
    "        if(save):\n",
    "            plt.savefig('./Imgs/'+str(i)+'.png')\n",
    "        else:\n",
    "            plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all(imgs, epoch, label, date, path= dir_checkpoint, experiment = exp_name):\n",
    "    images = imgs.detach().cpu().numpy()\n",
    "    images = np.transpose(images, axes=(0,2,3,1))\n",
    "    plt.axis('off')\n",
    "    try:\n",
    "        os.mkdir(path+'/'+experiment+str(date)+'/'+str(epoch))\n",
    "    except:\n",
    "        pass\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.imshow(images[i])\n",
    "        plt.savefig(path+'/'+experiment+str(date)+'/'+str(epoch)+'/'+str(label)+'_'+ str(i)+'.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close('all')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(net_GA)\n",
    "print(net_GB)\n",
    "print(net_adversarial_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 750 # depends upon the dataset/task\n",
    "resume = False\n",
    "adversarial_stage = 50 #  hyperparameter decides when the stage-2 begins. This stage depends upon the dataset/task\n",
    "save_epoch_freq = 10 # help='frequency of saving checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Implementation using Unet architecture\n",
    "#resume= True #h elp = set True to resume from the loaded checkpoints\n",
    "cdate = str(time.ctime())\n",
    "os.mkdir('./checkpoints/'+exp_name+cdate)\n",
    "print('Training started: ',time.ctime())\n",
    "start = time.time()\n",
    "\n",
    "dataloader_len = dataset_train_A.__len__()\n",
    "if (dataset_train_A.__len__() > dataset_train_B.__len__()):\n",
    "    dataloader_len = dataset_train_B.__len__()\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "\n",
    "    batch_DA_loss = 0\n",
    "    batch_DB_loss = 0\n",
    "    batch_D_common_loss = 0    \n",
    "    batch_GA_loss = 0\n",
    "    batch_GB_loss = 0\n",
    "    batch_D_common_GAN_1 = 0\n",
    "    batch_D_common_GAN_2 = 0\n",
    "    \n",
    "    batch_GA_GAN_loss = 0\n",
    "    batch_GB_GAN_loss = 0\n",
    "    batch_GA_L1_real_loss = 0\n",
    "    batch_GA_L1_fake_loss = 0\n",
    "    \n",
    "    batch_GB_L1_real_loss = 0\n",
    "    batch_GB_L1_fake_loss = 0\n",
    "\n",
    "    batch_GA_common_GAN = 0\n",
    "    batch_GB_common_GAN = 0\n",
    "\n",
    "    batch_loss_DA_fake_A1 = 0\n",
    "    batch_loss_DA_real_A1 = 0\n",
    "    batch_loss_DA_fake_B1 = 0\n",
    "    batch_loss_DA_fake_B2 = 0\n",
    "\n",
    "    batch_loss_DB_fake_A1 = 0\n",
    "    batch_loss_DB_fake_A2 = 0\n",
    "    batch_loss_DB_fake_B1 = 0\n",
    "    batch_loss_DB_real_B1 = 0\n",
    "\n",
    "    batch_GA_common_GAN_1 = 0\n",
    "    batch_GA_common_GAN_2 = 0\n",
    "    batch_GB_common_GAN_1 = 0\n",
    "    batch_GB_common_GAN_2 = 0\n",
    "    \n",
    "    batch_loss_D_common_fake_A = 0\n",
    "    batch_loss_D_common_real_A = 0\n",
    "    batch_loss_D_common_fake_B = 0\n",
    "    batch_loss_D_common_real_B = 0\n",
    "    \n",
    "    print('Starting epoch {}/{}.'.format(epoch + 1, n_epochs))\n",
    "\n",
    "    \n",
    "    real_images_A = torch.Tensor(batchsize, channels, img_size, img_size)\n",
    "    fake_images_A = torch.Tensor(batchsize, channels, img_size, img_size)\n",
    "    real_images_B = torch.Tensor(batchsize, channels, img_size, img_size)\n",
    "    fake_images_B = torch.Tensor(batchsize, channels, img_size, img_size)\n",
    "        \n",
    "    ip_images_A = torch.Tensor(batchsize, channels, img_size, img_size)\n",
    "    ip_images_B = torch.Tensor(batchsize, channels, img_size, img_size)\n",
    "        \n",
    "    dl_train_A = iter(dataloader_train_A)\n",
    "    dl_train_B = iter(dataloader_train_B)\n",
    "    \n",
    "    for i in range(dataloader_len//batchsize):\n",
    "        try:\n",
    "            imgs_1, target_1 =  next(dl_train_A)\n",
    "        except StopIteration:\n",
    "            dl_train_A = iter(dataloader_train_A)\n",
    "            imgs_1, target_1 =  next(dl_train_A)\n",
    "            \n",
    "        try:\n",
    "            imgs_2, target_2 =  next(dl_train_B)\n",
    "        except StopIteration:\n",
    "            dl_train_B = iter(dataloader_train_B)\n",
    "            imgs_2, target_2 =  next(dl_train_B)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for j in range(batchsize):\n",
    "                # generate random numbers to select images of the other domain\n",
    "                rand_num_1 = np.random.randint(batchsize)\n",
    "                rand_num_2 = np.random.randint(batchsize)\n",
    "\n",
    "                # prevent same images from appearing as real or fake\n",
    "                while(rand_num_1==j):\n",
    "                    rand_num_1 = np.random.randint(batchsize)\n",
    "                while(rand_num_2==j):\n",
    "                    rand_num_2 = np.random.randint(batchsize)\n",
    "                \n",
    "                real_images_A[j] = imgs_1[rand_num_1] # Domain A image. Real Image to Discriminator A e.g. Horse \n",
    "                fake_images_A[j] = imgs_2[rand_num_2] # Random Domain B image. Fake image to Discriminator A. e.g. Zebra\n",
    "                \n",
    "                real_images_B[j] = imgs_2[rand_num_1] # Domain B image, Real Image to Discriminator B e.g. Zebra \n",
    "                fake_images_B[j] = imgs_1[rand_num_2] # Random Domain B image. Fake image to Discriminator B. e.g. Horse\n",
    "\n",
    "                ip_images_A[j] = imgs_2[j] # Input to the generator A is Domain B image. e.g. Zebra \n",
    "                ip_images_B[j] = imgs_1[j] # Input to the generator B is Domain A image. e.g. Horse\n",
    "                                \n",
    "            if(cuda):\n",
    "                ip_images_A = ip_images_A.cuda()\n",
    "                ip_images_B = ip_images_B.cuda()\n",
    "                                \n",
    "                real_images_A = real_images_A.cuda()\n",
    "                fake_images_A = fake_images_A.cuda()\n",
    "                                \n",
    "                real_images_B = real_images_B.cuda() \n",
    "                fake_images_B = fake_images_B.cuda()\n",
    "\n",
    "            # compute fake images:\n",
    "            #ip_images_A = torch.cat((ip_images_A_1, ip_images_A_2), 2)\n",
    "            #ip_images_B = torch.cat((ip_images_B_1, ip_images_B_2), 2)\n",
    "    \n",
    "        #--------------------------------------------------------------------\n",
    "        #       Update Generator for Domain 1\n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        optimizer_GB.zero_grad()        # set GB's gradients to zero\n",
    "        optimizer_adversarial_1.zero_grad()        # set GA's gradients to zero\n",
    "        \n",
    "        # update GA\n",
    "        set_requires_grad(net_adversarial_1, True)  # DB requires no gradients when optimizing GA\n",
    "        set_requires_grad(net_GA, True)  # GA requires gradients when optimizing Generator for domain 1\n",
    "        set_requires_grad(net_GB, False)  # GB requires no gradients when optimizing GA\n",
    "        \n",
    "        optimizer_GA.zero_grad()        # set GA's gradients to zero\n",
    "\n",
    "        recon_images_A = net_GA(ip_images_A) # Construct the images from input images\n",
    "        \n",
    "        #Calculate GAN and L1 loss for the generator\n",
    "        \n",
    "        # Generator A should generate images that are not real for discriminator B \n",
    "        fake_A2 = torch.cat((ip_images_A, recon_images_A), 2)\n",
    "        pred_fake_A2_DB = net_adversarial_1(fake_A2)\n",
    "        loss_GA_GAN_DB = criterionGAN(pred_fake_A2_DB, True) * lambda_criterionGAN\n",
    "                \n",
    "        # Second, G(A) = B\n",
    "        loss_GA_L1_real = criterionL1(recon_images_A, ip_images_A) * lambda_L1_real\n",
    "                                \n",
    "        # combine loss and calculate gradients\n",
    "        loss_GA = (loss_GA_GAN_DB + loss_GA_L1_real) * 0.50\n",
    "        loss_GA.backward()\n",
    "\n",
    "        optimizer_GA.step()             # udpate G's weights\n",
    "        \n",
    "        batch_GA_GAN_loss += loss_GA_GAN_DB.item()\n",
    "        \n",
    "        batch_GA_L1_real_loss += loss_GA_L1_real.item()\n",
    "        batch_GA_loss += loss_GA.item()\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------\n",
    "        #       Update Generator for Domain 2\n",
    "        #--------------------------------------------------------------------\n",
    "        \n",
    "        optimizer_GA.zero_grad()        # set GA's gradients to zero\n",
    "        optimizer_adversarial_1.zero_grad()        # set GA's gradients to zero\n",
    "        \n",
    "        # update GB\n",
    "        set_requires_grad(net_adversarial_1, True)  # DB requires no gradients when optimizing GB\n",
    "        set_requires_grad(net_GA, False)  # GA requires no gradients when optimizing GB\n",
    "        set_requires_grad(net_GB, True)  # GB requires gradients when optimizing GB\n",
    "        \n",
    "        optimizer_GB.zero_grad()        # set GB's gradients to zero\n",
    "\n",
    "        recon_images_B = net_GB(ip_images_B)\n",
    "        \n",
    "        #Calculate GAN and L1 loss for the generator\n",
    "                \n",
    "        # First, G(A) should fake the discriminator\n",
    "        fake_B2 = torch.cat((ip_images_B, recon_images_B), 2)\n",
    "        pred_fake_B2_DB = net_adversarial_1(fake_B2)\n",
    "        loss_GB_GAN_DB = criterionGAN(pred_fake_B2_DB, False) * lambda_criterionGAN\n",
    "        \n",
    "        # Second, G(A) = B\n",
    "        loss_GB_L1_real = criterionL1(recon_images_B, ip_images_B) * lambda_L1_real\n",
    "                                \n",
    "        # combine loss and calculate gradients\n",
    "        loss_GB = (loss_GB_GAN_DB + loss_GB_L1_real) * 0.50\n",
    "        \n",
    "        loss_GB.backward()\n",
    "\n",
    "        optimizer_GB.step()             # udpate G's weights\n",
    "        \n",
    "        batch_GB_GAN_loss += loss_GB_GAN_DB.item()\n",
    "        \n",
    "        batch_GB_L1_real_loss += loss_GB_L1_real.item()\n",
    "        \n",
    "        batch_GB_loss += loss_GB.item()\n",
    "        \n",
    "        #--------------------------------------------------------------------\n",
    "        #       Update Domain 2 Discriminator \n",
    "        #--------------------------------------------------------------------\n",
    "        # realA real_images \n",
    "        # realB content images\n",
    "        # fakeB recon_images\n",
    "        \n",
    "        optimizer_GA.zero_grad()        # set GA's gradients to zero\n",
    "        optimizer_GB.zero_grad()        # set GA's gradients to zero\n",
    "        \n",
    "        # update DB\n",
    "        set_requires_grad(net_adversarial_1, True)  # DB requires gradients when optimizing Discriminator for domain B\n",
    "        set_requires_grad(net_GA, False)  # GA requires no gradients when optimizing DB\n",
    "        set_requires_grad(net_GB, False)  # GB requires no gradients when optimizing DB\n",
    "                \n",
    "        # enable backprop for D\n",
    "        optimizer_adversarial_1.zero_grad()     # set DB's gradients to zero\n",
    "\n",
    "        #Calculate GAN loss for the discriminator\n",
    "\n",
    "        # Domain B images are real for discriminator B\n",
    "        real_A1 = torch.cat((ip_images_A, fake_images_A), 2)\n",
    "        pred_real_A1_DB = net_adversarial_1(real_A1)\n",
    "        loss_DB_real_A1 = criterionGAN(pred_real_A1_DB, False) * lambda_criterionGAN\n",
    "\n",
    "        # Generator A images are fake for discriminator B        \n",
    "        fake_A1 = torch.cat((ip_images_A, recon_images_A.detach()), 2)\n",
    "        pred_fake_A1_DB = net_adversarial_1(fake_A1)\n",
    "        loss_DB_fake_A1 = criterionGAN(pred_fake_A1_DB, False) * lambda_criterionGAN\n",
    "        \n",
    "        # Domain A images are fake for discriminator B\n",
    "        fake_A2 = torch.cat((ip_images_A, real_images_A), 2)\n",
    "        pred_fake_A2_DB = net_adversarial_1(fake_A2)\n",
    "        loss_DB_fake_A2 = criterionGAN(pred_fake_A2_DB, True) * lambda_criterionGAN\n",
    "        \n",
    "        # Domain B images are real for discriminator B\n",
    "        real_B1 = torch.cat((ip_images_B, real_images_B), 2)\n",
    "        pred_real_B1_DB = net_adversarial_1(real_B1)\n",
    "        loss_DB_real_B1 = criterionGAN(pred_real_B1_DB, False) * lambda_criterionGAN\n",
    "\n",
    "        # Generator B images are fake for discriminator B\n",
    "        fake_B1 = torch.cat((ip_images_B, recon_images_B.detach()), 2)\n",
    "        pred_fake_B1_DB = net_adversarial_1(fake_B1)\n",
    "        loss_DB_fake_B1 = criterionGAN(pred_fake_B1_DB, True) * lambda_criterionGAN\n",
    "\n",
    "        # Domain A images are fake for discriminator B\n",
    "        fake_B2 = torch.cat((ip_images_B, fake_images_B), 2)\n",
    "        pred_fake_B2_DB = net_adversarial_1(fake_B2)\n",
    "        loss_DB_fake_B2 = criterionGAN(pred_fake_B2_DB, True) * lambda_criterionGAN\n",
    "\n",
    "        # combine loss and calculate gradients\n",
    "        loss_DB = (loss_DB_real_A1 + loss_DB_real_B1 + loss_DB_fake_A1 + loss_DB_fake_A2 + loss_DB_fake_B1 + loss_DB_fake_B2) * (1.0/6.0)\n",
    "        loss_DB.backward()\n",
    "        optimizer_adversarial_1.step()          # update D's weights\n",
    "\n",
    "        batch_loss_DB_fake_A1 += loss_DB_fake_A1.item()\n",
    "        batch_loss_DB_fake_A2 += loss_DB_fake_A2.item()\n",
    "        batch_loss_DB_fake_B1 += loss_DB_fake_B1.item()\n",
    "        batch_loss_DB_real_B1 += loss_DB_fake_B1.item()\n",
    "        \n",
    "        batch_DB_loss += loss_DB.item()\n",
    "        \n",
    "        #--------------------------------------------------------------------\n",
    "      \n",
    "        \n",
    "        if(i%50==0):\n",
    "            recon_imgs = torch.cat((recon_images_A, recon_images_B),2)\n",
    "            ip_imgs = torch.cat((ip_images_A, ip_images_B,),2)\n",
    "            imgs_display = torch.cat((ip_imgs, recon_imgs),3)\n",
    "            save_all(imgs_display,epoch,'real_recon_'+str(i),cdate)\n",
    "        \n",
    "    end_epoch = time.time()\n",
    "    epoch_time_sec = (end_epoch - start_epoch)\n",
    "    epoch_time_min = epoch_time_sec/60\n",
    "    print('Epoch',epoch,'time: ',int(epoch_time_min),'min',int(epoch_time_sec)%60,'s')\n",
    "    #print('D-loss:',batch_D_loss,'G_GAN-Loss:',batch_G_GAN_loss, 'G_L1-Loss:',batch_G_L1_loss, 'G_L2-Loss:',batch_G_L2_loss)\n",
    "    #print('D-loss:',batch_D_loss,'G_GAN-Loss:',batch_G_GAN_loss, 'G_L1_1-Loss:',batch_G_L1_1_loss, 'G_L1_2-Loss:',batch_G_L1_2_loss)\n",
    "    if(epoch % save_epoch_freq == 0):\n",
    "        torch.save(net_GA.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/net_GA_{}.pth'.format(epoch + 1))\n",
    "        torch.save(net_GB.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/net_GB_{}.pth'.format(epoch + 1))\n",
    "        print('Checkpoint {} saved !'.format(epoch + 1))        \n",
    "    if(epoch % save_epoch_freq*10 == 0):\n",
    "        torch.save(net_adversarial_1.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/net_adversarial_1_latest.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_adversarial_1.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/net_adversarial_1_{}.pth'.format(epoch + 1))\n",
    "torch.save(net_GA.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/net_GA_{}.pth'.format(epoch + 1))\n",
    "torch.save(net_GB.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/net_GB_{}.pth'.format(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(optimizer_adversarial_1.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/optimizer_adversarial_1_{}.pth'.format(epoch + 1))\n",
    "torch.save(optimizer_GA.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/optimizer_GA_{}.pth'.format(epoch + 1))\n",
    "torch.save(optimizer_GB.state_dict(), dir_checkpoint+'/'+exp_name+str(cdate)+'/optimizer_GB_{}.pth'.format(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For testing\n",
    "mode = 'test'\n",
    "if(mode)=='test':\n",
    "    n_epochs = 1\n",
    "    save_ip_and_recon = False\n",
    "    \n",
    "cdate = str(time.ctime())\n",
    "os.mkdir('./checkpoints/'+exp_name+cdate)\n",
    "print('Training started: ',time.ctime())\n",
    "start = time.time()\n",
    "\n",
    "start_checkpoint = 501\n",
    "end_checkpoint = 760\n",
    "checkpoint_interval = 10\n",
    "\n",
    "for checkpoint in range(start_checkpoint,end_checkpoint,checkpoint_interval):\n",
    "    for i in range(2):\n",
    "        test_recon_images = []\n",
    "        if(i%2 == 0):\n",
    "            model_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/net_GA_'+str(checkpoint)+'.pth')\n",
    "            net_GA.load_state_dict(model_saved)\n",
    "            model = net_GA\n",
    "            model.eval()\n",
    "            dl_test = iter(dataloader_test_B)\n",
    "            \n",
    "        else:\n",
    "            model_saved = torch.load('./checkpoints/'+exp_name+str(date_time)+'/net_GB_'+str(checkpoint)+'.pth')\n",
    "            net_GB.load_state_dict(model_saved)\n",
    "            model = net_GB\n",
    "            model.eval()\n",
    "            dl_test = iter(dataloader_test_A)\n",
    "    \n",
    "        start_epoch = time.time()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            print('Starting Test')\n",
    "            ip_images = torch.Tensor(batchsize, channels, img_size, img_size)\n",
    "\n",
    "            iterate = True\n",
    "            count = 0\n",
    "            while(iterate):\n",
    "                try:\n",
    "                    imgs, target =  next(dl_test)\n",
    "                    count += 1\n",
    "                except StopIteration:\n",
    "                    iterate = False\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for j in range(batchsize):\n",
    "                        ip_images[j] = imgs[j] # Input to the generator A is Domain B image. e.g. Zebra \n",
    "\n",
    "                    if(cuda):\n",
    "                        ip_images = ip_images.cuda()\n",
    "\n",
    "                recon_images = model(ip_images)\n",
    "                if(save_ip_and_recon):\n",
    "                    imgs_display = torch.cat((ip_images, recon_images),2)\n",
    "                else:\n",
    "                    imgs_display = recon_images.detach()\n",
    "                    test_recon_images.append(imgs_display)\n",
    "                    \n",
    "                save_all(imgs_display,str(checkpoint)+str(i),'real_recon_'+str(count),cdate)\n",
    "            \n",
    "            if(save_ip_and_recon):\n",
    "                dataset = np.array(test_recon_images)\n",
    "                dataset = dataset.reshape(-1,3,256,256)\n",
    "                #dataset = dataset.transpose(0,3,1,2)\n",
    "                np.save(str(checkpoint)+str(i),dataset)\n",
    "            \n",
    "            end_epoch = time.time()\n",
    "            epoch_time_sec = (end_epoch - start_epoch)\n",
    "            epoch_time_min = epoch_time_sec/60\n",
    "            print('Epoch',checkpoint,'time: ',int(epoch_time_min),'min',int(epoch_time_sec)%60,'s')\n",
    "\n",
    "            print('Complete !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from path\n",
    "def pil_loader(path):\n",
    "    return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset to dictionary files and save as pytorch files\n",
    "def dataset_to_dict_AB(dataset):\n",
    "    print('started',dataset)\n",
    "    start_save = time.time()\n",
    "    train_classes_A, train_images_A = dataset_memory_loader(dataset,ops=['trainA'])\n",
    "    train_classes_B, train_images_B = dataset_memory_loader(dataset,ops=['trainB'])\n",
    "    \n",
    "    test_classes_A, test_images_A = dataset_memory_loader(dataset,ops=['testA'])\n",
    "    test_classes_B, test_images_B = dataset_memory_loader(dataset,ops=['testB'])\n",
    "        \n",
    "    torch.save(dict(zip(train_classes_A,train_images_A)),str(dataset)+'_trainA.pt')\n",
    "    torch.save(dict(zip(train_classes_B,train_images_B)),str(dataset)+'_trainB.pt')\n",
    "    \n",
    "    torch.save(dict(zip(test_classes_A,test_images_A)),str(dataset)+'_testA.pt')\n",
    "    torch.save(dict(zip(test_classes_B,test_images_B)),str(dataset)+'_testB.pt')\n",
    "\n",
    "    end_save = time.time()\n",
    "    print(dataset,'completed in',int(end_save-start_save),'s')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def dataset_memory_loader(task, ops, dir_data  = './'):\n",
    "    for op in ops:\n",
    "        images = []\n",
    "        file_names = []\n",
    "        path = dir_data+task+'/'+op\n",
    "        print(path)\n",
    "        count = 0\n",
    "        for file in os.listdir(path):\n",
    "            img_path = path+'/'+file\n",
    "            file_names.append(img_path)\n",
    "            img = pil_loader(img_path)\n",
    "            img_int = np.array(img,dtype = 'uint8')\n",
    "            images.append(img_int)\n",
    "        count = count + 1\n",
    "        return file_names, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_dict_AB('horse2zebra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose network\n",
    "def diagnose_network(net, name='network'):\n",
    "    \"\"\"Calculate and print the mean of average absolute(gradients)\n",
    "\n",
    "    Parameters:\n",
    "        net (torch network) -- Torch network\n",
    "        name (str) -- the name of the network\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    count = 0\n",
    "    for param in net.parameters():\n",
    "        if param.grad is not None:\n",
    "            mean += torch.mean(torch.abs(param.grad.data))\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        mean = mean / count\n",
    "    print(name)\n",
    "    print(mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
